{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eace2c9",
   "metadata": {},
   "source": [
    "# Baseline Experiment: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "336d5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e368fc",
   "metadata": {},
   "source": [
    "# Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161577c",
   "metadata": {},
   "source": [
    "# Define Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration for data paths and model parameters.\n",
    "    \"\"\"\n",
    "    DATA_PATH = Path(\"data.csv\")\n",
    "    SAMPLE_SIZE = 500\n",
    "    TEST_SIZE = 0.25\n",
    "    RANDOM_STATE = 42\n",
    "    MAX_FEATURES = 100\n",
    "    MODEL_MAX_ITER = 1000\n",
    "    MLFLOW_URI = 'https://dagshub.com/SurajBhar/moviesentiment.mlflow'\n",
    "    EXPERIMENT_NAME = 'Logistic Regression Baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec143a6",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d88bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample_data(path: Path, sample_size: int, random_state: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from CSV and return a random sample.\n",
    "\n",
    "    Args:\n",
    "        path (Path): Path to the CSV file.\n",
    "        sample_size (int): Number of samples to draw.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    sampled = df.sample(sample_size, random_state=random_state)\n",
    "    sampled.to_csv(path, index=False)\n",
    "    logging.info(f\"Loaded and sampled {sample_size} rows.\")\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f1a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 16:19:08,711 - INFO - Loaded and sampled 500 rows.\n"
     ]
    }
   ],
   "source": [
    "df = load_and_sample_data(Config.DATA_PATH, Config.SAMPLE_SIZE, Config.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81a317",
   "metadata": {},
   "source": [
    "# The dataset contains 2 main Features\n",
    "- Review\n",
    "- Sentiment\n",
    "- Size of the dataset: 500x2\n",
    "- This dataset is a subset generated from original IMDB dataset.\n",
    "- Due to Computation Limits we are going to use the small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c304fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>There wasn't a 0 in the voting option so i was...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>When I was 11, Grease 2 was like crack. It was...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Some amusing humor, some that falls flat, some...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>I can say without a shadow of a doubt that Goi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Attack Force has a horrendous title, and can a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Now, I like sci-fi cartoons. However, when \"Ro...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>If you want to be cynical and pedantic you cou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Trapped: buried alive brings us to a resort th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>I recently stumbled across a TV showing of \"Pa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Growing up with the Beast Wars transformers, I...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "361  There wasn't a 0 in the voting option so i was...  negative\n",
       "73   When I was 11, Grease 2 was like crack. It was...  negative\n",
       "374  Some amusing humor, some that falls flat, some...  negative\n",
       "155  I can say without a shadow of a doubt that Goi...  negative\n",
       "104  Attack Force has a horrendous title, and can a...  negative\n",
       "..                                                 ...       ...\n",
       "106  Now, I like sci-fi cartoons. However, when \"Ro...  negative\n",
       "270  If you want to be cynical and pedantic you cou...  positive\n",
       "348  Trapped: buried alive brings us to a resort th...  positive\n",
       "435  I recently stumbled across a TV showing of \"Pa...  positive\n",
       "102  Growing up with the Beast Wars transformers, I...  negative\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07ec35",
   "metadata": {},
   "source": [
    "# Normalization Steps\n",
    "\n",
    "**1. Lowercasing**\n",
    "\n",
    "- What it does:\n",
    "    - Convert all characters in your text to lowercase (e.g., ‚ÄúThe Quick Brown Fox‚Äù ‚Üí ‚Äúthe quick brown fox‚Äù).\n",
    "\n",
    "- Why it helps:\n",
    "\n",
    "    - Vocabulary reduction: ‚ÄúDog,‚Äù ‚Äúdog,‚Äù and ‚ÄúDOG‚Äù all become ‚Äúdog,‚Äù so your model learns fewer, more robust word forms.\n",
    "\n",
    "    - Consistency: Downstream tokenizers and embeddings treat words uniformly, improving statistics (e.g., word counts, TF‚ÄìIDF).\n",
    "\n",
    "**2. Remove URLs**\n",
    "- What it does:\n",
    "    - Strip out web addresses (e.g., ‚Äúhttps://example.com/page‚Äù). Typically done via a regex like https?://\\S+ or <http\\S+>.\n",
    "\n",
    "- Why it helps:\n",
    "\n",
    "    - Reduce noise: URLs rarely contain linguistically meaningful content for most tasks (unless you‚Äôre doing specialized link analysis).\n",
    "\n",
    "    - Prevent data leakage: Raw URLs can embed user-specific or sensitive information.\n",
    "\n",
    "**3. Remove Numbers**\n",
    "- What it does:\n",
    "    - Delete or replace digit sequences (e.g., ‚ÄúIn 2025, sales hit 1,000‚Äù ‚Üí ‚ÄúIn , sales hit ,‚Äù). Often achieved with a regex like \\d+.\n",
    "\n",
    "- Why it helps:\n",
    "\n",
    "    - Normalize quantities: Unless your task needs exact numeric values (e.g., financial forecasting), stripping numbers keeps focus on the textual patterns.\n",
    "\n",
    "    - Vocabulary control: Prevents proliferation of unique ‚Äúwords‚Äù like ‚Äú2025,‚Äù ‚Äú2024,‚Äù etc.\n",
    "\n",
    "**4. Remove Punctuation**\n",
    "- What it does:\n",
    "    - Strip punctuation marks (.,!?;:‚Äú‚Äù()-‚Äî‚Ä¶) from the text.\n",
    "\n",
    "- Why it helps:\n",
    "\n",
    "    - Cleaner tokens: Ensures ‚Äúhello,‚Äù ‚Äúhello!‚Äù and ‚Äúhello?‚Äù all map to ‚Äúhello.‚Äù\n",
    "\n",
    "    - Simplify modeling: Many algorithms treat punctuation as separate tokens, which often adds little semantic value in typical text classification or topic modeling.\n",
    "\n",
    "**5. Remove Stop Words**\n",
    "- What it does:\n",
    "    - Eliminate very frequent, semantically-light words like ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúand,‚Äù ‚Äúof.‚Äù Common stop‚Äëword lists include 100‚Äì300 words.\n",
    "\n",
    "- Why it helps:\n",
    "\n",
    "    - Focus on content words: Drops words that carry little topic or sentiment information.\n",
    "\n",
    "    - Dimensionality reduction: Fewer tokens ‚Üí smaller vocabulary ‚Üí faster training.\n",
    "\n",
    "**6. Lemmatization**\n",
    "- What it does:\n",
    "    - Reduce words to their dictionary (lemma) form, taking context and part-of-speech into account (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun,‚Äù ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù).\n",
    "\n",
    "- Why it helps:\n",
    "\n",
    "    - Semantic grouping: ‚Äúrun,‚Äù ‚Äúruns,‚Äù ‚Äúrunning,‚Äù ‚Äúran‚Äù all become ‚Äúrun,‚Äù so your model sees them as the same concept.\n",
    "\n",
    "    - Improved generalization: Helps with sparsity, especially in smaller datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3816adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> str:\n",
    "    \"\"\"\n",
    "    Apply normalization steps: lowercase, remove URLs, numbers, punctuation, stop words, and lemmatize.\n",
    "\n",
    "    Args:\n",
    "        text (str): Original text review.\n",
    "        lemmatizer (WordNetLemmatizer): NLTK lemmatizer.\n",
    "        stop_words (set): Set of stop words.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove numbers\n",
    "    text = ''.join(ch for ch in text if not ch.isdigit())\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    # Tokenize & remove stop words\n",
    "    tokens = [w for w in text.split() if w not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmed = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return ' '.join(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ff69a",
   "metadata": {},
   "source": [
    "# Normalize the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de286bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize the 'review' column in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a 'review' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned reviews.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['review'] = df['review'].apply(lambda txt: preprocess_text(txt, lemmatizer, stop_words))\n",
    "    logging.info(\"Completed text normalization.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502f4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary NLTK resources are available\n",
    "for pkg in ['stopwords', 'wordnet', 'omw-1.4']:\n",
    "    try:\n",
    "        nltk.data.find(f'corpora/{pkg}')\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3239f95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 16:19:42,748 - INFO - Completed text normalization.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Normalize text\n",
    "df = normalize_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2365981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>voting option compelled use next available fig...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>grease like crack classless shameful euphoric ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>amusing humor fall flat decent acting quite at...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>say without shadow doubt going overboard singl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>attack force horrendous title almost certainly...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>like sci fi cartoon however robotboy appeared ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>want cynical pedantic could point opening raf ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>trapped buried alive brings u resort opened so...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>recently stumbled across tv showing passion mi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>growing beast war transformer familiar origina...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "361  voting option compelled use next available fig...  negative\n",
       "73   grease like crack classless shameful euphoric ...  negative\n",
       "374  amusing humor fall flat decent acting quite at...  negative\n",
       "155  say without shadow doubt going overboard singl...  negative\n",
       "104  attack force horrendous title almost certainly...  negative\n",
       "..                                                 ...       ...\n",
       "106  like sci fi cartoon however robotboy appeared ...  negative\n",
       "270  want cynical pedantic could point opening raf ...  positive\n",
       "348  trapped buried alive brings u resort opened so...  positive\n",
       "435  recently stumbled across tv showing passion mi...  positive\n",
       "102  growing beast war transformer familiar origina...  negative\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30647559",
   "metadata": {},
   "source": [
    "# Transform text data into features\n",
    "- Maps the Sentiments into (positive:1, negative:0)\n",
    "\n",
    "- We are‚ÄØusing scikit‚Äëlearn‚Äôs `CountVectorizer` to turn a column of raw text (‚Äúreviews‚Äù) into a numeric feature matrix. Let‚Äôs break it down:\n",
    "\n",
    "1. **`CountVectorizer(max_features=max_features)`**\n",
    "\n",
    "   * **What it is:** A transformer that converts a collection of text documents into a ‚Äúbag‚Äëof‚Äëwords‚Äù count matrix.\n",
    "   * **`max_features` parameter:** Limits the vocabulary to the top‚ÄØ`max_features` most frequent tokens across your entire corpus. This helps control dimensionality by discarding all but the most common words.\n",
    "\n",
    "2. **`.fit_transform(df['review'])`**\n",
    "\n",
    "   * **`fit`:**\n",
    "\n",
    "     * Learns the vocabulary dictionary: it scans all the reviews, tokenizes them (by default on whitespace and punctuation), counts word frequencies, and selects the top‚ÄØ`max_features` words to keep.\n",
    "   * **`transform`:**\n",
    "\n",
    "     * Converts each review into a vector of word counts, where each dimension corresponds to one of the selected vocabulary words.\n",
    "   * **Combining them (`fit_transform`):**\n",
    "\n",
    "     * Efficiently does both steps in one go: first it ‚Äúfits‚Äù to build the vocabulary, then immediately ‚Äútransforms‚Äù the input text into the count matrix.\n",
    "\n",
    "3. **Result `X`**\n",
    "\n",
    "   * A **sparse matrix** of shape `(n_samples, max_features)`.\n",
    "\n",
    "     * **`n_samples`** = number of rows in `df['review']`.\n",
    "     * **`max_features`** = number of columns (the size of your truncated vocabulary).\n",
    "   * Each row is a document (one review); each column is a token (one of the top‚ÄØ`max_features` words).\n",
    "   * The entries are integer counts: how many times each word appears in that review.\n",
    "\n",
    "---\n",
    "\n",
    "### Why use this?\n",
    "\n",
    "* **Numerical input for ML models:** Most classical algorithms (logistic regression, SVMs, random forests, etc.) expect numeric arrays, not raw text.\n",
    "* **Dimensionality control:** By capping at `max_features`, you avoid extremely high‚Äëdimensional matrices that blow up memory and overfit on rare words.\n",
    "* **Baseline representation:** Bag‚Äëof‚Äëwords is a simple yet surprisingly effective way to capture word‚Äëfrequency information as features.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose:\n",
    "\n",
    "```python\n",
    "df['review'] = [\n",
    "    \"I loved the movie, it was fantastic!\",\n",
    "    \"Terrible acting and boring plot.\",\n",
    "    \"Fantastic visuals but boring story.\"\n",
    "]\n",
    "max_features = 5\n",
    "```\n",
    "\n",
    "After fitting, `CountVectorizer` might pick these top‚ÄØ5 words:\n",
    "\n",
    "```\n",
    "['boring', 'fantastic', 'movie', 'story', 'terrible']\n",
    "```\n",
    "\n",
    "Then `X.toarray()` would look something like:\n",
    "\n",
    "```python\n",
    "array([\n",
    "  [0, 1, 1, 0, 0],   # \"loved the movie fantastic\" ‚Üí counts: boring=0, fantastic=1, movie=1, ...\n",
    "  [1, 0, 0, 0, 1],   # \"terrible acting boring plot\" ‚Üí boring=1, terrible=1\n",
    "  [1, 1, 0, 1, 0],   # \"fantastic visuals boring story\" ‚Üí boring=1, fantastic=1, story=1\n",
    "])\n",
    "```\n",
    "\n",
    "Each row is a review, each column a word count for one of the top features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19c890a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df: pd.DataFrame, max_features: int):\n",
    "    \"\"\"\n",
    "    Transform text data into features and split into train/test sets.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test, vectorizer\n",
    "    \"\"\"\n",
    "    df = df[df['sentiment'].isin(['positive', 'negative'])].copy()\n",
    "    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(df['review'])\n",
    "    y = df['sentiment'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=Config.TEST_SIZE, random_state=Config.RANDOM_STATE\n",
    "    )\n",
    "    logging.info(\"Prepared train/test splits.\")\n",
    "    return X_train, X_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83af1a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 16:39:52,268 - INFO - Prepared train/test splits.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare features\n",
    "X_train, X_test, y_train, y_test, vectorizer = prepare_features(df, Config.MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcca5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X-train: (375, 100)\n",
      "The shape of X-test: (125, 100)\n",
      "The shape of y-train: (375,)\n",
      "The shape of y-test: (125,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of X-train: {X_train.shape}\")\n",
    "print(f\"The shape of X-test: {X_test.shape}\")\n",
    "print(f\"The shape of y-train: {y_train.shape}\")\n",
    "print(f\"The shape of y-test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01bd26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 8 stored elements and shape (1, 100)>\n",
      "  Coords\tValues\n",
      "  (0, 12)\t2\n",
      "  (0, 62)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 44)\t1\n",
      "  (0, 77)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 19)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71794d4",
   "metadata": {},
   "source": [
    "# Inspect the features and transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb91dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acting' 'actor' 'actually' 'almost' 'also' 'another' 'around' 'back'\n",
      " 'bad' 'best' 'better' 'book' 'br' 'character' 'come' 'comedy' 'could'\n",
      " 'day' 'director' 'done' 'end' 'even' 'ever' 'every' 'fact' 'family' 'fan'\n",
      " 'feel' 'film' 'find' 'first' 'get' 'give' 'go' 'going' 'good' 'great'\n",
      " 'guy' 'horror' 'interesting' 'know' 'life' 'like' 'line' 'little' 'long'\n",
      " 'look' 'looking' 'lot' 'love' 'made' 'make' 'man' 'many' 'minute' 'movie'\n",
      " 'much' 'must' 'never' 'new' 'nothing' 'old' 'one' 'part' 'people'\n",
      " 'performance' 'play' 'plot' 'pretty' 'quite' 'real' 'really' 'role' 'say'\n",
      " 'scene' 'see' 'seems' 'seen' 'show' 'something' 'star' 'still' 'story'\n",
      " 'take' 'thing' 'think' 'though' 'time' 'two' 'want' 'watch' 'watching'\n",
      " 'way' 'well' 'whole' 'woman' 'work' 'world' 'would' 'year']\n"
     ]
    }
   ],
   "source": [
    "# 1. Grab the feature names from your vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "373ab45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Turn the first N rows of X_train into a dense array + DataFrame\n",
    "N = 5\n",
    "X_sample = X_train[:N].toarray()                # shape (N, num_features)\n",
    "df_X_sample = pd.DataFrame(X_sample, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "134e284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Attach the matching y_train labels\n",
    "df_X_sample['sentiment'] = y_train[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8ebbbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing first 5 training samples (bag-of-words counts + label):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acting</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>almost</th>\n",
       "      <th>also</th>\n",
       "      <th>another</th>\n",
       "      <th>around</th>\n",
       "      <th>back</th>\n",
       "      <th>bad</th>\n",
       "      <th>best</th>\n",
       "      <th>...</th>\n",
       "      <th>watching</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>whole</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acting  actor  actually  almost  also  another  around  back  bad  best  \\\n",
       "0       0      0         0       0     0        0       0     0    0     0   \n",
       "1       0      1         0       0     0        1       0     0    0     1   \n",
       "2       1      0         3       1     1        0       1     0    1     0   \n",
       "3       0      1         0       0     1        0       0     0    0     0   \n",
       "4       0      0         0       0     0        0       0     0    0     0   \n",
       "\n",
       "   ...  watching  way  well  whole  woman  work  world  would  year  sentiment  \n",
       "0  ...         0    0     0      0      0     0      0      0     0          0  \n",
       "1  ...         0    0     0      0      0     0      0      1     0          0  \n",
       "2  ...         1    0     1      0      1     0      0      0     0          1  \n",
       "3  ...         0    0     0      0      0     1      0      1     0          1  \n",
       "4  ...         0    0     0      0      0     0      0      0     0          0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Display\n",
    "print(f\"Showing first {N} training samples (bag-of-words counts + label):\")\n",
    "display(df_X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94defe9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/surajbhardwaj/opt/anaconda3/envs/senti/lib/python3.10/site-packages/rich/live.py:229: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/surajbhardwaj/opt/anaconda3/envs/senti/lib/python3.10/site-packages/rich/live.py:229: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=7a4ffb2f-e1fd-4606-b0eb-abd31a264241&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=1500b982910b420330bc1887c8d4a4ca1b7a8c3a888103ffddc3727896e41df6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as SurajBhar\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as SurajBhar\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 16:50:41,437 - INFO - Accessing as SurajBhar\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"SurajBhar/moviesentiment\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"SurajBhar/moviesentiment\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 16:50:41,750 - INFO - Initialized MLflow to track repo \"SurajBhar/moviesentiment\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository SurajBhar/moviesentiment initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository SurajBhar/moviesentiment initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 16:50:41,755 - INFO - Repository SurajBhar/moviesentiment initialized!\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "mlflow.set_tracking_uri(Config.MLFLOW_URI)\n",
    "dagshub.init(repo_owner='SurajBhar', repo_name='moviesentiment', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7459681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(X_train, y_train, X_test, y_test):  # noqa: C901\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression model, evaluate it, and log parameters/metrics with MLflow.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features.\n",
    "        y_train: Training labels.\n",
    "        X_test: Test features.\n",
    "        y_test: Test labels.\n",
    "    \"\"\"\n",
    "    # Configure MLflow\n",
    "    mlflow.set_tracking_uri(Config.MLFLOW_URI)\n",
    "    mlflow.set_experiment(Config.EXPERIMENT_NAME)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Log feature extraction parameters\n",
    "        mlflow.log_param(\"vectorizer\", \"CountVectorizer\")\n",
    "        mlflow.log_param(\"max_features\", Config.MAX_FEATURES)\n",
    "        mlflow.log_param(\"test_size\", Config.TEST_SIZE)\n",
    "\n",
    "        # Initialize and train model\n",
    "        model = LogisticRegression(max_iter=Config.MODEL_MAX_ITER)\n",
    "        model.fit(X_train, y_train)\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1_score\": f1_score(y_test, y_pred),\n",
    "        }\n",
    "        for name, val in metrics.items():\n",
    "            mlflow.log_metric(name, val)\n",
    "        logging.info(f\"Evaluation metrics: {metrics}\")\n",
    "\n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"Run completed in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "816a4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/30 16:51:20 INFO mlflow.tracking.fluent: Experiment with name 'Logistic Regression Baseline' does not exist. Creating a new experiment.\n",
      "2025-07-30 16:51:23,473 - INFO - Evaluation metrics: {'accuracy': 0.72, 'precision': 0.7090909090909091, 'recall': 0.6724137931034483, 'f1_score': 0.6902654867256637}\n",
      "2025/07/30 16:51:33 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2025-07-30 16:51:33,629 - INFO - Run completed in 11.89s\n",
      "2025/07/30 16:51:33 INFO mlflow.tracking._tracking_service.client: üèÉ View run silent-stork-949 at: https://dagshub.com/SurajBhar/moviesentiment.mlflow/#/experiments/0/runs/5dc250f01505466ca4489e3fa4b0d85c.\n",
      "2025/07/30 16:51:33 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/SurajBhar/moviesentiment.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train and log model\n",
    "train_and_log_model(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
